{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AI4Bharat/indicTrans/blob/main/indictrans_fairseq_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P0uptOB6U7GW",
    "outputId": "988c867e-76ee-4a54-a232-e69abbc5c3db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/maharaj/repositories/experiments/indicTrans/testing\n"
     ]
    }
   ],
   "source": [
    "# create a seperate folder to store everything\n",
    "!mkdir testing\n",
    "%cd testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kQFRiLtSalzt",
    "outputId": "03070c7c-8299-46bf-de56-df09c3213a3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'indicTrans'...\n",
      "remote: Enumerating objects: 694, done.\u001b[K\n",
      "remote: Counting objects: 100% (397/397), done.\u001b[K\n",
      "remote: Compressing objects: 100% (201/201), done.\u001b[K\n",
      "remote: Total 694 (delta 273), reused 272 (delta 194), pack-reused 297\u001b[K\n",
      "Receiving objects: 100% (694/694), 2.65 MiB | 4.28 MiB/s, done.\n",
      "Resolving deltas: 100% (400/400), done.\n",
      "/home/maharaj/repositories/experiments/indicTrans/testing/indicTrans\n",
      "Cloning into 'indic_nlp_library'...\n",
      "remote: Enumerating objects: 1362, done.\u001b[K\n",
      "remote: Counting objects: 100% (143/143), done.\u001b[K\n",
      "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
      "remote: Total 1362 (delta 111), reused 98 (delta 93), pack-reused 1219\u001b[K\n",
      "Receiving objects: 100% (1362/1362), 9.56 MiB | 5.93 MiB/s, done.\n",
      "Resolving deltas: 100% (721/721), done.\n",
      "Cloning into 'indic_nlp_resources'...\n",
      "remote: Enumerating objects: 139, done.\u001b[K\n",
      "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
      "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
      "remote: Total 139 (delta 2), reused 2 (delta 0), pack-reused 126\u001b[K\n",
      "Receiving objects: 100% (139/139), 149.77 MiB | 7.36 MiB/s, done.\n",
      "Resolving deltas: 100% (53/53), done.\n",
      "Cloning into 'subword-nmt'...\n",
      "remote: Enumerating objects: 597, done.\u001b[K\n",
      "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
      "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
      "remote: Total 597 (delta 8), reused 12 (delta 4), pack-reused 576\u001b[K\n",
      "Receiving objects: 100% (597/597), 252.23 KiB | 3.66 MiB/s, done.\n",
      "Resolving deltas: 100% (357/357), done.\n",
      "/home/maharaj/repositories/experiments/indicTrans/testing\n"
     ]
    }
   ],
   "source": [
    "# clone the repo for running evaluation\n",
    "!git clone https://github.com/AI4Bharat/indicTrans.git\n",
    "%cd indicTrans\n",
    "# clone requirements repositories\n",
    "!git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git\n",
    "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
    "!git clone https://github.com/rsennrich/subword-nmt.git\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FHUQGCACVvVf",
    "outputId": "67c7c3a0-f8bf-46a2-8214-e36556df989b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacremoses in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (0.0.53)\n",
      "Requirement already satisfied: pandas in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (1.5.0)\n",
      "Requirement already satisfied: mock in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (4.0.3)\n",
      "Requirement already satisfied: sacrebleu in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (2.2.1)\n",
      "Requirement already satisfied: tensorboardX in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (2.5.1)\n",
      "Requirement already satisfied: pyarrow in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (9.0.0)\n",
      "Requirement already satisfied: indic-nlp-library in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (0.81)\n",
      "Requirement already satisfied: six in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from sacremoses) (1.16.0)\n",
      "Requirement already satisfied: click in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from sacremoses) (8.1.3)\n",
      "Requirement already satisfied: joblib in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from sacremoses) (1.2.0)\n",
      "Requirement already satisfied: regex in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from sacremoses) (2022.9.13)\n",
      "Requirement already satisfied: tqdm in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from sacremoses) (4.64.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from pandas) (2022.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from pandas) (1.23.1)\n",
      "Requirement already satisfied: colorama in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from sacrebleu) (0.4.5)\n",
      "Requirement already satisfied: portalocker in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from sacrebleu) (2.5.1)\n",
      "Requirement already satisfied: lxml in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from sacrebleu) (4.9.1)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from tensorboardX) (3.20.1)\n",
      "Requirement already satisfied: sphinx-argparse in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from indic-nlp-library) (0.3.2)\n",
      "Requirement already satisfied: morfessor in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from indic-nlp-library) (2.0.6)\n",
      "Requirement already satisfied: sphinx-rtd-theme in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from indic-nlp-library) (1.0.0)\n",
      "Requirement already satisfied: sphinx>=1.2.0 in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from sphinx-argparse->indic-nlp-library) (5.2.3)\n",
      "Requirement already satisfied: docutils<0.18 in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from sphinx-rtd-theme->indic-nlp-library) (0.17.1)\n",
      "Requirement already satisfied: babel>=2.9 in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.10.3)\n",
      "Requirement already satisfied: imagesize>=1.3 in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
      "Requirement already satisfied: sphinxcontrib-jsmath in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
      "Requirement already satisfied: sphinxcontrib-applehelp in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.2)\n",
      "Requirement already satisfied: requests>=2.5.0 in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.28.1)\n",
      "Requirement already satisfied: sphinxcontrib-devhelp in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.8 in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (4.11.4)\n",
      "Requirement already satisfied: packaging>=21.0 in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (21.3)\n",
      "Requirement already satisfied: sphinxcontrib-qthelp in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.3)\n",
      "Requirement already satisfied: snowballstemmer>=2.0 in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
      "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.1.5)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.1.2)\n",
      "Requirement already satisfied: alabaster<0.8,>=0.7 in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.7.12)\n",
      "Requirement already satisfied: Pygments>=2.12 in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.13.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from importlib-metadata>=4.8->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.8.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from Jinja2>=3.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from packaging>=21.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.3)\n",
      "Cloning into 'fairseq'...\n",
      "remote: Enumerating objects: 34534, done.\u001b[K\n",
      "remote: Total 34534 (delta 0), reused 0 (delta 0), pack-reused 34534\u001b[K\n",
      "Receiving objects: 100% (34534/34534), 24.06 MiB | 8.26 MiB/s, done.\n",
      "Resolving deltas: 100% (25109/25109), done.\n",
      "/home/maharaj/repositories/experiments/indicTrans/testing/fairseq\n",
      "Processing /home/maharaj/repositories/experiments/indicTrans/testing/fairseq\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.21.3 in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from fairseq==0.12.2) (1.23.1)\n",
      "Requirement already satisfied: cython in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from fairseq==0.12.2) (0.29.32)\n",
      "Requirement already satisfied: tqdm in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from fairseq==0.12.2) (4.64.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sacrebleu>=1.4.12 in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from fairseq==0.12.2) (2.2.1)\n",
      "Requirement already satisfied: hydra-core<1.1,>=1.0.7 in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from fairseq==0.12.2) (1.0.7)\n",
      "Requirement already satisfied: torchaudio>=0.8.0 in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from fairseq==0.12.2) (0.12.1)\n",
      "Requirement already satisfied: regex in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from fairseq==0.12.2) (2022.9.13)\n",
      "Requirement already satisfied: bitarray in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from fairseq==0.12.2) (2.6.0)\n",
      "Collecting torch>=1.13\n",
      "Collecting xformers\n",
      "  Downloading xformers-0.0.16-cp39-cp39-manylinux2014_x86_64.whl (50.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyre-extensions==0.0.23\n",
      "  Downloading pyre_extensions-0.0.23-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: numpy in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from xformers) (1.23.1)\n",
      "Collecting torch==1.13.1\n",
      "  Downloading torch-1.13.1-cp39-cp39-manylinux1_x86_64.whl (887.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.4/887.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:03\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspect\n",
      "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
      "Requirement already satisfied: typing-extensions in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from pyre-extensions==0.0.23->xformers) (4.3.0)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Requirement already satisfied: setuptools in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->xformers) (63.4.1)\n",
      "Requirement already satisfied: wheel in /home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->xformers) (0.37.1)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, mypy-extensions, typing-inspect, nvidia-cudnn-cu11, torch, pyre-extensions, xformers\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.1\n",
      "    Uninstalling torch-1.12.1:\n",
      "      Successfully uninstalled torch-1.12.1\n",
      "Successfully installed mypy-extensions-1.0.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 pyre-extensions-0.0.23 torch-1.13.1 typing-inspect-0.8.0 xformers-0.0.16\n",
      "/home/maharaj/repositories/experiments/indicTrans/testing\n"
     ]
    }
   ],
   "source": [
    "# Install the necessary libraries\n",
    "!pip install sacremoses pandas mock sacrebleu tensorboardX pyarrow indic-nlp-library\n",
    "# Install fairseq from source\n",
    "!git clone https://github.com/pytorch/fairseq.git\n",
    "%cd fairseq\n",
    "# !git checkout da9eaba12d82b9bfc1442f0e2c6fc1b895f4d35d\n",
    "!pip install ./\n",
    "! pip install xformers\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "L6SGeKxq1XPI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/maharaj/anaconda3/envs/pyt/lib/python3.9/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2023-03-18 10:59:41 | WARNING | xformers | Triton is not available, some optimizations will not be enabled.\n",
      "This is just a warning: No module named 'triton'\n"
     ]
    }
   ],
   "source": [
    "# add fairseq folder to python path\n",
    "import os\n",
    "# os.environ['PYTHONPATH'] += \":/content/fairseq/\"\n",
    "# sanity check to see if fairseq is installed\n",
    "from fairseq import checkpoint_utils, distributed_utils, options, tasks, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kKA8afhBawO5",
    "outputId": "d346f462-d5d4-43a0-c29b-90aaab2fb4d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-18 11:37:58--  https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/m2m.tar\n",
      "Resolving ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)... 101.53.152.33, 101.53.136.18, 164.52.210.96, ...\n",
      "Connecting to ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)|101.53.152.33|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5236582400 (4.9G) [application/x-tar]\n",
      "Saving to: ‘m2m.tar’\n",
      "\n",
      "m2m.tar               2%[                    ] 124.73M  6.74MB/s    in 3m 23s  \n",
      "\n",
      "2023-03-18 11:41:22 (629 KB/s) - Connection closed at byte 130792973. Retrying.\n",
      "\n",
      "--2023-03-18 11:41:23--  (try: 2)  https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/m2m.tar\n",
      "Connecting to ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)|101.53.152.33|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 5236582400 (4.9G), 5105789427 (4.8G) remaining [application/x-tar]\n",
      "Saving to: ‘m2m.tar’\n",
      "\n",
      "m2m.tar               3%[                    ] 149.93M  --.-KB/s    in 19m 8s  \n",
      "\n",
      "2023-03-18 12:00:31 (22.5 KB/s) - Read error at byte 157216768/5236582400 (Connection timed out). Retrying.\n",
      "\n",
      "--2023-03-18 12:00:33--  (try: 3)  https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/m2m.tar\n",
      "Connecting to ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)|101.53.152.33|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 5236582400 (4.9G), 5079365632 (4.7G) remaining [application/x-tar]\n",
      "Saving to: ‘m2m.tar’\n",
      "\n",
      "m2m.tar              77%[==============>     ]   3.80G  7.77MB/s    in 15m 38s \n",
      "\n",
      "2023-03-18 12:16:11 (3.99 MB/s) - Connection closed at byte 4079214026. Retrying.\n",
      "\n",
      "--2023-03-18 12:16:14--  (try: 4)  https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/m2m.tar\n",
      "Connecting to ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)|101.53.152.33|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 5236582400 (4.9G), 1157368374 (1.1G) remaining [application/x-tar]\n",
      "Saving to: ‘m2m.tar’\n",
      "\n",
      "m2m.tar              80%[+++++++++++++++>    ]   3.92G  5.51MB/s    in 99s     \n",
      "\n",
      "2023-03-18 12:17:53 (1.27 MB/s) - Connection closed at byte 4210277267. Retrying.\n",
      "\n",
      "--2023-03-18 12:17:57--  (try: 5)  https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/m2m.tar\n",
      "Connecting to ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)|101.53.152.33|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 5236582400 (4.9G), 1026305133 (979M) remaining [application/x-tar]\n",
      "Saving to: ‘m2m.tar’\n",
      "\n",
      "m2m.tar              82%[++++++++++++++++    ]   4.04G  5.15MB/s    in 3m 25s  \n",
      "\n",
      "2023-03-18 12:21:22 (630 KB/s) - Connection closed at byte 4342587392. Retrying.\n",
      "\n",
      "--2023-03-18 12:21:27--  (try: 6)  https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/m2m.tar\n",
      "Connecting to ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)|101.53.152.33|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 5236582400 (4.9G), 893995008 (853M) remaining [application/x-tar]\n",
      "Saving to: ‘m2m.tar’\n",
      "\n",
      "m2m.tar              92%[++++++++++++++++=>  ]   4.50G  4.39MB/s    in 3m 28s  \n",
      "\n",
      "2023-03-18 12:24:56 (2.23 MB/s) - Connection closed at byte 4830019584. Retrying.\n",
      "\n",
      "--2023-03-18 12:25:02--  (try: 7)  https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/m2m.tar\n",
      "Connecting to ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)|101.53.152.33|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 5236582400 (4.9G), 406562816 (388M) remaining [application/x-tar]\n",
      "Saving to: ‘m2m.tar’\n",
      "\n",
      "m2m.tar              95%[++++++++++++++++++> ]   4.66G  5.16MB/s    in 4m 25s  \n",
      "\n",
      "2023-03-18 12:29:27 (645 KB/s) - Connection closed at byte 5004922314. Retrying.\n",
      "\n",
      "--2023-03-18 12:29:34--  (try: 8)  https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/m2m.tar\n",
      "Connecting to ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)|101.53.152.33|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 5236582400 (4.9G), 231660086 (221M) remaining [application/x-tar]\n",
      "Saving to: ‘m2m.tar’\n",
      "\n",
      "m2m.tar              98%[+++++++++++++++++++ ]   4.78G  3.96MB/s    in 4m 10s  \n",
      "\n",
      "2023-03-18 12:33:43 (512 KB/s) - Connection closed at byte 5135674260. Retrying.\n",
      "\n",
      "--2023-03-18 12:33:51--  (try: 9)  https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/m2m.tar\n",
      "Connecting to ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)|101.53.152.33|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 5236582400 (4.9G), 100908140 (96M) remaining [application/x-tar]\n",
      "Saving to: ‘m2m.tar’\n",
      "\n",
      "m2m.tar             100%[+++++++++++++++++++>]   4.88G  4.33MB/s    in 4m 13s  \n",
      "\n",
      "2023-03-18 12:38:04 (390 KB/s) - ‘m2m.tar’ saved [5236582400/5236582400]\n",
      "\n",
      "m2m/\n",
      "m2m/vocab/\n",
      "m2m/vocab/bpe_codes.32k.SRC\n",
      "m2m/vocab/vocab.SRC\n",
      "m2m/vocab/vocab.TGT\n",
      "m2m/vocab/bpe_codes.32k.SRC_TGT\n",
      "m2m/vocab/bpe_codes.32k.TGT\n",
      "m2m/final_bin/\n",
      "m2m/final_bin/dict.TGT.txt\n",
      "m2m/final_bin/dict.SRC.txt\n",
      "m2m/model/\n",
      "m2m/model/checkpoint_best.pt\n",
      "[Errno 2] No such file or directory: 'indicTrans'\n",
      "/home/maharaj/repositories/experiments/indicTrans/testing/indicTrans\n"
     ]
    }
   ],
   "source": [
    "# download the indictrans model\n",
    "\n",
    "\n",
    "# downloading the indic-en model\n",
    "!wget https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/indic2en.zip\n",
    "!unzip indic2en.zip\n",
    "\n",
    "# downloading the en-indic model\n",
    "!wget https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/en2indic.zip\n",
    "!unzip en2indic.zip\n",
    "\n",
    "# # downloading the indic-indic model\n",
    "!wget https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/m2m.tar\n",
    "!tar -xvf m2m.tar\n",
    "\n",
    "%cd indicTrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Lg1sQFfyWJli"
   },
   "outputs": [],
   "source": [
    "# creating a text file and adding en sentences we can use for testing the model\n",
    "!touch en_sentences.txt\n",
    "!echo 'This bicycle is too small for you !!' >> en_sentences.txt\n",
    "!echo \"I will directly meet you at the airport.\" >> en_sentences.txt\n",
    "!echo 'If COVID-19 is spreading in your community, stay safe by taking some simple precautions, such as physical distancing, wearing a mask, keeping rooms well ventilated, avoiding crowds, cleaning your hands, and coughing into a bent elbow or tissue' >> en_sentences.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fLg9BWAGWvLU",
    "outputId": "f3ca6f65-9a39-4d80-c25d-88806daf3e7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 9 15:18:01 UTC 2021\n",
      "Applying normalization and script conversion\n",
      "100% 3/3 [00:00<00:00, 71.78it/s]\n",
      "Number of sentences in input: 3\n",
      "Applying BPE\n",
      "Decoding\n",
      "Extracting translations, script conversion and detokenization\n",
      "Translation completed\n"
     ]
    }
   ],
   "source": [
    "# joint_translate takes src_file, output_fname, src_lang, tgt_lang, model_folder as inputs\n",
    "# src_file -> input text file to be translated\n",
    "# output_fname -> name of the output file (will get created) containing the model predictions\n",
    "# src_lang -> source lang code of the input text ( in this case we are using en-indic model and hence src_lang would be 'en')\n",
    "# tgt_lang -> target lang code of the input text ( tgt lang for en-indic model would be any of the 11 indic langs we trained on:\n",
    "#              as, bn, hi, gu, kn, ml, mr, or, pa, ta, te)\n",
    "# supported languages are:\n",
    "#              as - assamese, bn - bengali, gu - gujarathi, hi - hindi, kn - kannada, \n",
    "#              ml - malayalam, mr - marathi, or - oriya, pa - punjabi, ta - tamil, te - telugu\n",
    "\n",
    "# model_dir -> the directory containing the model and the vocab files\n",
    "\n",
    "# Note: if the translation is taking a lot of time, please tune the buffer_size and batch_size parameter for fairseq-interactive defined inside this joint_translate script\n",
    "\n",
    "\n",
    "# here we are translating the english sentences to tamil\n",
    "!bash joint_translate.sh en_sentences.txt ta_outputs.txt 'en' 'ta' '../en-indic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8QzkBCgeGZiH",
    "outputId": "c150360c-6d01-4689-8c2e-9bdd0eba1504"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat ta_outputs.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4v9BmbZao5d",
    "outputId": "6efac2a3-5f79-4e72-821b-bc80702a7fa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 9 15:21:31 UTC 2021\n",
      "Applying normalization and script conversion\n",
      "100% 3/3 [00:00<00:00, 88.59it/s]\n",
      "Number of sentences in input: 3\n",
      "Applying BPE\n",
      "Decoding\n",
      "Extracting translations, script conversion and detokenization\n",
      "Translation completed\n"
     ]
    }
   ],
   "source": [
    "# Similarly, we can translate the english sentences to hindi\n",
    "!bash joint_translate.sh en_sentences.txt hi_outputs.txt 'en' 'hi' '../en-indic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pNNzyR_LfqIr",
    "outputId": "095b9532-e76a-4451-dec9-4862566a4288"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "यह साइकिल तुम्हारे लिए बहुत छोटी है!\n",
      "मैं आपसे एयरपोर्ट पर ही मिलने वाला हूं।\n",
      "यदि आपके समुदाय में कोविड-19 फैल रहा है, तो कुछ सरल सावधानियां बरतें, जैसे शारीरिक दूरी बनाए रखना, मास्क पहनना, कमरों को अच्छी तरह से हवादार रखना, भीड़ से बचना, अपने हाथों को साफ करना और कोहनी या ऊतक को मोड़कर खांसते हुए सुरक्षित रहें\n"
     ]
    }
   ],
   "source": [
    "!cat hi_outputs.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PzjbDLBtaol9"
   },
   "outputs": [],
   "source": [
    "# creating a text file and adding hi sentences we can use for testing the model\n",
    "!touch hi_sentences.txt\n",
    "!echo 'तुम आज सुबह यहाँ क्यों आए?' >> hi_sentences.txt\n",
    "!echo \"मेरे परिवार में हर कोई जल्दी उठता है।\" >> hi_sentences.txt\n",
    "!echo ' स्वास्थ्य और परिवार कल्याण मंत्रालय द्वारा प्रदान की गई जानकारी और सलाह को सावधानी व सही तरीके से पालन कर वायरस के स्थानीय प्रसार को रोका जा सकता है।' >> hi_sentences.txt\n",
    "\n",
    "!touch ta_sentences.txt\n",
    "!echo 'அவனுக்கு நம்மைப் தெரியும் என்று தோன்றுகிறது' >> ta_sentences.txt\n",
    "!echo \"இது எங்கே இருக்கு என்று என்னால் கண்டுபிடிக்க முடியவில்லை.\" >> ta_sentences.txt\n",
    "!echo 'உங்களுக்கு உங்கள் அருகில் இருக்கும் ஒருவருக்கோ இத்தகைய அறிகுறிகள் தென்பட்டால், வீட்டிலேயே இருப்பது, கொரோனா வைரஸ் தொற்று பிறருக்கு வராமல் தடுக்க உதவும்.' >> ta_sentences.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5uaOmKb8gmeN",
    "outputId": "951bbdf9-61d0-4703-a8df-0c3fcb4e5bb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 9 15:24:43 UTC 2021\n",
      "Applying normalization and script conversion\n",
      "100% 3/3 [00:00<00:00, 74.90it/s]\n",
      "Number of sentences in input: 3\n",
      "Applying BPE\n",
      "Decoding\n",
      "Extracting translations, script conversion and detokenization\n",
      "Translation completed\n"
     ]
    }
   ],
   "source": [
    "# here we are translating the english sentences to hindi\n",
    "!bash joint_translate.sh hi_sentences.txt en_outputs.txt 'hi' 'en' '../indic-en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iLD7WPqmlSnC",
    "outputId": "359050fa-6d35-4055-a9c5-13a15322c59e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did you come here this morning?\n",
      "Everyone in my family gets up early.\n",
      "The local spread of the virus can be curbed by following the information and advice provided by the Ministry of Health and Family Welfare in a careful and correct manner.\n"
     ]
    }
   ],
   "source": [
    "! cat en_outputs.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O3mJyj-QljWz",
    "outputId": "1c0420e5-4b80-41d9-f09e-2fdff79bc7bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 9 15:28:05 UTC 2021\n",
      "Applying normalization and script conversion\n",
      "100% 3/3 [00:00<00:00, 72.92it/s]\n",
      "Number of sentences in input: 3\n",
      "Applying BPE\n",
      "Decoding\n",
      "Extracting translations, script conversion and detokenization\n",
      "Translation completed\n"
     ]
    }
   ],
   "source": [
    "# here we are translating the english sentences to tamil\n",
    "!bash joint_translate.sh ta_sentences.txt en_outputs.txt 'ta' 'en' '../indic-en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GapEJESiloD8",
    "outputId": "dc8b2a8c-4f36-4bf9-d517-6826aa65da57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He seems to know us.\n",
      "I couldnt find it anywhere.\n",
      "If someone in your neighbourhood develops these symptoms, staying at home can help prevent the spread of the coronavirus infection.\n"
     ]
    }
   ],
   "source": [
    "! cat en_outputs.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckfW2P6abcB3"
   },
   "outputs": [],
   "source": [
    "# we just rename the m2m_joint_vocab file here as joint_translate uses bpe_codes.32k.SRC\n",
    "mv ../m2m/vocab/bpe_codes.32k.SRC_TGT ../m2m/vocab/bpe_codes.32k.SRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H-3vPdCqSWoK",
    "outputId": "d5a80c59-cc89-4910-a9ce-7317fac6bf8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 9 15:39:26 UTC 2021\n",
      "Applying normalization and script conversion\n",
      "100% 3/3 [00:00<00:00, 63.53it/s]\n",
      "Number of sentences in input: 3\n",
      "Applying BPE\n",
      "Decoding\n",
      "Extracting translations, script conversion and detokenization\n",
      "Translation completed\n"
     ]
    }
   ],
   "source": [
    "# here we are using the indic2indic model for translating the hindi sentences to tamil\n",
    "!bash joint_translate.sh hi_sentences.txt ta_outputs.txt 'hi' 'ta' '../m2m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22yPo78Zb_oR",
    "outputId": "4df17e93-9029-4020-8deb-0dbaf8bb0b27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "तुम आज सुबह यहाँ क्यों आए?\n",
      "मेरे परिवार में हर कोई जल्दी उठता है।\n",
      " स्वास्थ्य और परिवार कल्याण मंत्रालय द्वारा प्रदान की गई जानकारी और सलाह को सावधानी व सही तरीके से पालन कर वायरस के स्थानीय प्रसार को रोका जा सकता है।\n"
     ]
    }
   ],
   "source": [
    " ! cat hi_sentences.txt # the hindi inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "onnfzTDESg2I",
    "outputId": "1bc600d4-d3ff-40fa-d258-7d1c876bd49c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ஏன் இன்று காலையில் வந்தீர்கள்?\n",
      "எனது குடும்பத்தில் உள்ள ஒவ்வொருவரும் விரைவில் எழுவார்கள்.\n",
      "மத்திய சுகாதாரம் மற்றும் குடும்ப நல அமைச்சகத்தின் அறிவுறுத்தல்கள் மற்றும் தகவல்களைப் பின்பற்றுவதன் மூலம், உள்ளூர் அளவில் வைரஸ் பரவுவதைத் தடுக்க முடியும்.\n"
     ]
    }
   ],
   "source": [
    "! cat ta_outputs.txt # the tamil outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5klOcwi8SjGS",
    "outputId": "bc4e47fa-ee1d-4da2-85ea-f7900cae7b48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 9 15:45:53 UTC 2021\n",
      "Applying normalization and script conversion\n",
      "100% 3/3 [00:00<00:00, 82.25it/s]\n",
      "Number of sentences in input: 3\n",
      "Applying BPE\n",
      "Decoding\n",
      "Extracting translations, script conversion and detokenization\n",
      "Translation completed\n"
     ]
    }
   ],
   "source": [
    "# here we are using the indic2indic model for translating the hindi sentences to tamil (same as above with reversing the direction)\n",
    "!bash joint_translate.sh ta_sentences.txt hi_outputs.txt 'ta' 'hi' '../m2m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ifZhGkKc6oo",
    "outputId": "a0112e2b-a54b-48ad-e3ae-a3d84c6d097e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "அவனுக்கு நம்மைப் தெரியும் என்று தோன்றுகிறது\n",
      "இது எங்கே இருக்கு என்று என்னால் கண்டுபிடிக்க முடியவில்லை.\n",
      "உங்களுக்கு உங்கள் அருகில் இருக்கும் ஒருவருக்கோ இத்தகைய அறிகுறிகள் தென்பட்டால், வீட்டிலேயே இருப்பது, கொரோனா வைரஸ் தொற்று பிறருக்கு வராமல் தடுக்க உதவும்.\n"
     ]
    }
   ],
   "source": [
    "! cat ta_sentences.txt # the tamil inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v0x0YrWYSwwK",
    "outputId": "4c37d699-5b8e-4ae7-9724-953d7e165035"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ऐसा लगता है कि वह हमें जानता है।\n",
      "मुझे पता नहीं था कि यह कहां है।\n",
      "अगर आपके आस-पास के किसी व्यक्ति में ऐसे लक्षण दिखाई देते हैं, तो घर पर रहने से कोरोना वायरस को फैलने से रोकने में मदद मिलेगी।\n"
     ]
    }
   ],
   "source": [
    "! cat hi_outputs.txt   # the hi outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-xcnDOc4gNKC"
   },
   "outputs": [],
   "source": [
    "# to compute bleu scores for the predicitions with a reference file, use the following command\n",
    "\n",
    "# bash compute_bleu.sh pred_fname ref_fname src_lang tgt_lang\n",
    "# arguments:\n",
    "# pred_fname: file that contains model predictions\n",
    "# ref_fname: file that contains references\n",
    "# src_lang and tgt_lang : the source and target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9YK2BdwvrUgI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "indictrans_fairseq_inference.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "3c7d4130300118f0c7487d576c6841c0dbbdeec039e1e658ac9b107412a09af0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
